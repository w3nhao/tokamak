{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tokamak/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named actions expected length 12200 but got length 12100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     42\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/public/dengwenhao/tokamak_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mconsolidate_npz_files_to_hf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mconsolidate_npz_files_to_hf_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     29\u001b[0m targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_targets, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Create Hugging Face dataset\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/datasets/arrow_dataset.py:931\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    929\u001b[0m     arrow_typed_mapping[col] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    930\u001b[0m mapping \u001b[38;5;241m=\u001b[39m arrow_typed_mapping\n\u001b[0;32m--> 931\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/datasets/table.py:757\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pydict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    743\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m        `datasets.table.Table`\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/pyarrow/table.pxi:1920\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/pyarrow/table.pxi:6137\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/pyarrow/table.pxi:4740\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/pyarrow/table.pxi:4092\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokamak/lib/python3.8/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named actions expected length 12200 but got length 12100"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_0 = np.load(\"/public/dengwenhao/tokamak_data/1.npz\", allow_pickle=True)[\"data\"].item()\n",
    "\n",
    "def consolidate_npz_files_to_hf_dataset(path):\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    all_inputs, all_outputs, all_actions, all_targets = [], [], [], []\n",
    "\n",
    "    k = 0\n",
    "    max_files = 100\n",
    "    for file_name in os.listdir(path):\n",
    "        if file_name.endswith('.npz'):\n",
    "            data = np.load(os.path.join(path, file_name), allow_pickle=True)[\"data\"].item()\n",
    "            all_inputs.append(data[\"inputs\"])\n",
    "            all_outputs.append(data[\"outputs\"])\n",
    "            all_actions.append(data[\"actions\"])\n",
    "            all_targets.append(data[\"targets\"])\n",
    "            k += 1\n",
    "            if k >= max_files:\n",
    "                break\n",
    "\n",
    "    # Concatenate all data\n",
    "    inputs = np.concatenate(all_inputs, axis=0)\n",
    "    outputs = np.concatenate(all_outputs, axis=0)\n",
    "    actions = np.concatenate(all_actions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Create Hugging Face dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"inputs\": inputs,\n",
    "        \"outputs\": outputs,\n",
    "        \"actions\": actions,\n",
    "        \"targets\": targets\n",
    "    })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "path = \"/public/dengwenhao/tokamak_data/\"\n",
    "dataset = consolidate_npz_files_to_hf_dataset(path)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0[\"targets\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokamak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
